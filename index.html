<!doctype html>
<html xmlns:vertical-align="http://www.w3.org/1999/xhtml">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>IEP 2018-06-21</title>
		<meta name="author" content="Chris Barnes">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/janelia.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--Colours-->
		<script>
			HHMIBlue = "#058d96";
			HHMIBlack = "#000";
			HHMIGreenA = "#8ac341";
			HHMIGreenB = "#2da948";
			HHMIWhite = "#fff";
			HHMIGrayA = "#838286";
			HHMIGrayB = "#aaa";
			HHMIGrayC = "#dcdcdc";
		</script>

	</head>
	<body>
		<div class="reveal">
			<!--<header></header>-->
			<div class="slides">

<!--SLIDES START HERE-->

<section class="title-slide" data-background="linear-gradient(to right, #058d96, #8ac341)">
	<div style="display: table; width: 100%; margin-bottom: 20%">
		<div style="width: 50%; display: table-cell; vertical-align: middle">
			<img class="plain" src="img/HHMI_Janelia_White.svg"
				 style="background: none; max-width: 70%; min-height: 50%; vertical-align: middle;
				 width: auto; height: auto; padding: 0; margin: 0;">
		</div>

		<div style="width: 50%; display: table-cell; vertical-align: middle; padding: 0; margin: 0;">
			<h3 id="author" style="text-align: right"></h3>
			<h3 style="text-align: right">Cardona lab</h3>
			<!--<h3 id="date" style="text-align: right; vertical-align: text-bottom; padding: 0; margin: 0;">2018-06-21</h3>-->
		</div>
	</div>
	<h1 id="title"></h1>
	<h2 id="subtitle"></h2>
</section>

<section>
	<section data-markdown="slides/introduction/intro.md"></section>
</section>

<section>
	<section data-markdown="slides/extrasynaptic_network/intro.md"></section>

	<section>
		<figure>
			<img src="img/fig/hive_plot.png" width="70%">
			<figcaption class="citation"><a href="https://doi.org/10.1371/journal.pcbi.1005283">Bentley et al. (2016).
				The Multilayer Connectome of Caenorhabditis elegans</a></figcaption>
		</figure>

		<aside class="notes">
			hive plot
			<ul>
				<li>nodes are plotted on 3 axes: sensory, inter, motor</li>
				<li>length up axis is node degree</li>
				<li>connections are (undirected) edges</li>
				<li>Green are possible extrasynaptic connections with monoamines</li>
				<li>highly-connected sensory and motor neurons are dominated by extrasensory connections
				which don't show up in EM, profound effects on neuromodulation</li>
			</ul>
		</aside>
	</section>
</section>

<section>
	<section data-markdown="slides/tools/intro.md"></section>
	<section data-markdown="slides/tools/miscellaneous.md"></section>
	<section>
		<img src="img/fig/sdv.gif">
	</section>
	<section data-markdown="slides/tools/contributions.md"></section>
	<section data-markdown="slides/tools/l1_export.md"></section>
	<section>
		<img src="img/fig/n5_orthoviews.png" alt="Orthoviews of N5 stack in CATMAID" border="0">
		<aside class="notes">
			<p>H2N5 library which I contributed to, but primarily written by Andrew Champion.</p>
			<p>Not fast enough for the majority of tracing</p>
		</aside>
	</section>
	<section data-markdown="slides/tools/catpy.md"></section>
	<section data-markdown="slides/tools/catmaid_extensions.md"></section>
</section>

<section>
	<section data-markdown="slides/synapse_suggestor/intro.md"></section>

	<section data-markdown="slides/synapse_suggestor/synapse_detection.md"></section>

	<section>
		<img src="img/fig/synapse_thresholding.svg">
		<aside class="notes">
			<p>The question is how accurate the detections are.</p>

			<p>On the right is a precision-recall curve, and it shows that without post-hoc filtering
			we have a very high recall but a low precision (lots of false positives).</p>

			<p>We have statistics on the synapses' size and shape very easily accessible from this regime,
			so the PR curve shows what happens if we filter the synapses by those statistics
			(synaptic volume, contact volume, Z depth, certainty - a percentage output by the pixel classifier),
			and by a logistic regression classifier based on those features.</p>

			<p>Along with the violin plot, we can clearly toss out a lot of false positives with simple thresholding,
			but there are babies in that bathwater.
			At least an understanding of these characteristics would allow a user to pick
			only the most likely new synapses for manual review.</p>
		</aside>
	</section>

	<section data-markdown="slides/synapse_suggestor/catmaid_extension.md"></section>

	<section>
		<iframe
				width="854" height="480" src="https://www.youtube.com/embed/NGbmXV09r9g" frameborder="0"
				allow="autoplay; encrypted-media" allowfullscreen
		></iframe>
		<aside class="notes">

		</aside>
	</section>

	<section>
		<img src="img/fig/raw_pred_widget.png" alt="Debug view" border="0">
	</section>

	<section data-markdown="slides/synapse_suggestor/partner_classification.md"></section>

	<section>
		<img src="img/fig/data_sample.svg" alt="Sample training data for partner classification">
		<p style="background: #000000">
			<span style="color: #ff0000">membrane </span>
			<span style="color: #00ff00">raw </span>
			<span style="color: #0000ff">synapse</span>
		</p>
		<aside class="notes">
			<p>Firstly, we generate the data.</p>

			<p>There is a lot of literature already on classifying RGB images, so I wanted to pack our
				information cached earlier into 3 channels.</p>

			<p>I blacked out regions outside of the synapse and partner we're interested in,
				rotated it so that the partner is above the synapse, and fixed the synapse in the middle-bottom of the image.</p>

			<ul>
				<li>Presynaptic: dense vesicle cloud, T-bar (note segmentation failure)</li>
				<li>Postsynaptic: hard to see, but post-synaptic densities</li>
				<li>None: ambiguity between membrane and synapse, only corner of synapse</li>
			</ul>
		</aside>
	</section>

	<section>
		<img src="img/fig/training_accuracy.svg" alt="Validation accuracy during training">
		<aside class="notes">
			<p>Unfortunately, it didn't pan out - over training epochs we'd hope to see the validation accuracy increasing
			from chance (33%) towards 100%.</p>

			<p>The ResNet comprises a number of different architectures, primarily distinguished by the number of layers,
			but this was pretty much the story for all the ones I tried.</p>

			<p>Humans can do this problem, so I'd like to go back and make a more intelligent attempt if time allows -
			for example, it doesn't look like the membrane or synapse channels really add much,
			and the blacking out may be too strong a signal.</p>
		</aside>
	</section>

</section>

<section>
	<section data-markdown="slides/cleft_analysis/intro.md"></section>
	<section>
		<img src="img/fig/orn-pn-ch-basin.png" alt="ORN -> PN and a1 cho -> Basin">
		<aside class="notes">
			I've been blocked by technological issues which have been resolved this
			week, so in the mean time I manually annotated all of the synapses
			in the first projection layer in two well-studied bilateral circuits,
			mechanosensory chordotonal -> basin in the VNC, and in ORN -> PN in two
			classes.
		</aside>
	</section>
	<section data-markdown="slides/cleft_analysis/asymmetry.md"></section>
	<section>
		<img src="img/fig/cho_basin_leftright_bias.svg">
		<aside class="notes">
			<ul>
				<li>Comparing mirror pairs of neuron-neuron edges (made up of several synapses)</li>
				<li>fig1 Y axis: left count divided by total count - 0.5 * 2: 0 is perfect symmetry, 1 or -1 is unilateral</li>
				<li>fig2 y axis: mean of that as absolute</li>
				<li>Expect to see orange bars smaller than blue bars</li>
				<li>We don't - this is borne out in the ORN-PN connections too</li>
				<li>Extension: this test for bilateral projections to the same neuron (probably in the brain)</li>
			</ul>
		</aside>
	</section>
	<section>
		<img src="img/fig/cho_basin_count_vs_area.svg" width="80%">
		<aside class="notes">
			<ul>
				<li>Step through:</li>
				<li>look at points, assuming they're all the same colour</li>
				<li>Linear relationship between synapse count and total area</li>
				<li>Very close to 0 intercept: even very low count edges do not make larger synapses</li>
				<li>Connections between left/right pairs show that this trend is consistent</li>
			</ul>
		</aside>
	</section>
	<section>
		<img src="img/fig/cho_basin_synaptic_area_hist.svg">
		<aside class="notes">
			<ul>
				<li>approximately log normal distribution</li>
				<li>common for measurements of living tissue, as growth is often proportional to size
				 and therefore additive on a log scale</li>
			</ul>
		</aside>
	</section>
	<section>
		<img src="img/fig/orn_pn_synaptic_area_hist.svg">
		<aside class="notes">
			<ul>
				<li>Same thing for ORN-PN</li>
				<li>Median synapse is slightly larger.</li>
				<li>Higher variance (but smaller sample size)</li>
			</ul>
		</aside>
	</section>
	<section data-markdown="slides/cleft_analysis/extensions.md"></section>
	<section>
		<figure>
			<img src="img/fig/synapse_fractions.png">
			<figcaption class="citation"><a href="https://doi.org/10.7554/eLife.14859">Berck et al. (2016)
				The wiring diagram of a glomerular olfactory system</a></figcaption>
		</figure>
		<aside class="notes">
			synaptic fraction

			<ul>
				<li>synaptic contact number may not be as important as synaptic fraction -
					that is, the proportion of a receiver's inputs which come from a single
					sender, or vice versa</li>
				<li>remarkable parity between left-right pairs of
					olfactory glomerular local interneurons</li>
				<li>These account for some disparities in raw contact number</li>
				<li>Synaptic area may further equalise the remaining disparities</li>
				<li>Haven't done it yet because dendritic fields are large
					and every synapse needs to be labelled for this analysis to work
					- tractable with automation</li>
			</ul>
		</aside>
	</section>
</section>

<!--SLIDES END HERE-->

			</div>
		</div>

		<script>
			const titleNode = document.getElementById("title");
			if (titleNode && !titleNode.innerHTML) {
				 titleNode.innerHTML = document.title;
			}

			const authorNode = document.getElementById("author");
			if (authorNode && !authorNode.innerHTML) {
				 for (let meta of document.getElementsByTagName("meta")) {
					  if (meta.getAttribute("name") === "author") {
							authorNode.innerHTML = meta.getAttribute("content");
							break;
					  }
				 }
			}

			const dateNode = document.getElementById("date");
			if (dateNode && !dateNode.innerHTML) {
				 const today = new Date();
				 dateNode.innerHTML = `${today.getFullYear()}-${today.getMonth() + 1}-${today.getDate()}`
			}
		</script>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				transitionSpeed: "fast",
				progress: true,
				controlsTutorial: true,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
