<!doctype html>
<html xmlns:vertical-align="http://www.w3.org/1999/xhtml">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>IEP 2018-06-21</title>
		<meta name="author" content="Chris Barnes">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/janelia.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--Colours-->
		<script>
			HHMIBlue = "#058d96";
			HHMIBlack = "#000";
			HHMIGreenA = "#8ac341";
			HHMIGreenB = "#2da948";
			HHMIWhite = "#fff";
			HHMIGrayA = "#838286";
			HHMIGrayB = "#aaa";
			HHMIGrayC = "#dcdcdc";
		</script>

	</head>
	<body>
		<div class="reveal">
			<!--<header></header>-->
			<div class="slides">

<!--SLIDES START HERE-->

<section class="title-slide" data-background="linear-gradient(to right, #058d96, #8ac341)">
	<div style="display: table; width: 100%; margin-bottom: 20%">
		<div style="width: 50%; display: table-cell; vertical-align: middle">
			<img class="plain" src="img/HHMI_Janelia_White.svg"
				 style="background: none; max-width: 70%; min-height: 50%; vertical-align: middle;
				 width: auto; height: auto; padding: 0; margin: 0;">
		</div>

		<div style="width: 50%; display: table-cell; vertical-align: middle; padding: 0; margin: 0;">
			<h3 id="author" style="text-align: right"></h3>
			<h3 style="text-align: right">Cardona lab</h3>
			<!--<h3 id="date" style="text-align: right; vertical-align: text-bottom; padding: 0; margin: 0;">2018-06-21</h3>-->
		</div>
	</div>
	<h1 id="title"></h1>
	<h2 id="subtitle"></h2>
</section>

<section>
	<section data-markdown="slides/introduction/intro.md"></section>
	<section data-markdown="slides/introduction/slide1.md"></section>
</section>

<section>
	<section data-markdown="slides/extrasynaptic_network/intro.md"></section>
</section>

<section>
	<section data-markdown="slides/tools/intro.md"></section>
	<section data-markdown="slides/tools/miscellaneous.md"></section>
	<section data-markdown="slides/tools/contributions.md"></section>
	<section data-markdown="slides/tools/l1_export.md"></section>
	<section>
		<img src="img/fig/n5_orthoviews.png" alt="Orthoviews of N5 stack in CATMAID" border="0">
		<aside class="notes">
			<p>H2N5 library which I contributed to, but primarily written by Andrew Champion.</p>
			<p>Not fast enough for the majority of tracing</p>
		</aside>
	</section>
	<section data-markdown="slides/tools/catpy.md"></section>
	<section data-markdown="slides/tools/catmaid_extensions.md"></section>
</section>

<section>
	<section data-markdown="slides/synapse_suggestor/intro.md"></section>
	<section data-markdown="slides/synapse_suggestor/synapse_detection.md"></section>
	<section>
		<img src="img/fig/synapse_thresholding.svg">
		<aside class="notes">
			<p>The question is how accurate the detections are.</p>

			<p>On the right is a precision-recall curve, and it shows that without post-hoc filtering
			we have a very high recall but a low precision (lots of false positives).</p>

			<p>We have statistics on the synapses' size and shape very easily accessible from this regime,
			so the PR curve shows what happens if we filter the synapses by those statistics
			(synaptic volume, contact volume, Z depth, certainty - a percentage output by the pixel classifier),
			and by a logistic regression classifier based on those features.</p>

			<p>Along with the violin plot, we can clearly toss out a lot of false positives with simple thresholding,
			but there are babies in that bathwater.
			At least an understanding of these characteristics would allow a user to pick
			only the most likely new synapses for manual review.</p>
		</aside>
	</section>
	<section data-markdown="slides/synapse_suggestor/catmaid_extension.md"></section>
	<section>
		<iframe
				width="854" height="480" src="https://www.youtube.com/embed/NGbmXV09r9g" frameborder="0"
				allow="autoplay; encrypted-media" allowfullscreen
		></iframe>
		<aside class="notes">

		</aside>
	</section>
	<section>
		<img src="img/fig/raw_pred_widget.png" alt="Debug view" border="0">
	</section>
	<section data-markdown="slides/synapse_suggestor/partner_classification.md"></section>
	<section>
		<img src="img/fig/data_sample.svg" alt="Sample training data for partner classification">
		<p style="background: #000000">
			<span style="color: #ff0000">membrane </span>
			<span style="color: #00ff00">raw </span>
			<span style="color: #0000ff">synapse</span>
		</p>
		<aside class="notes">
			<p>Firstly, we generate the data.</p>

			<p>There is a lot of literature already on classifying RGB images, so I wanted to pack our
				information cached earlier into 3 channels.</p>

			<p>I blacked out regions outside of the synapse and partner we're interested in,
				rotated it so that the partner is above the synapse, and fixed the synapse in the middle-bottom of the image.</p>

			<ul>
				<li>Presynaptic: dense vesicle cloud, T-bar (note segmentation failure)</li>
				<li>Postsynaptic: hard to see, but post-synaptic densities</li>
				<li>None: ambiguity between membrane and synapse, only corner of synapse</li>
			</ul>
		</aside>
	</section>
	<section>
		<img src="img/fig/training_accuracy.svg" alt="Validation accuracy during training">
		<aside class="notes">
			Unfortunately, it didn't pan out - over training epochs we'd hope to see the validation accuracy increasing
			from chance (33%) towards 100%.

			The ResNet comprises a number of different architectures, primarily distinguished by the number of layers,
			but this was pretty much the story for all the ones I tried.

			Humans can do this problem, so I'd like to go back and make a more intelligent attempt if time allows -
			for example, it doesn't look like the membrane or synapse channels really add much,
			and the blacking out may be too strong a signal.
		</aside>
	</section>
</section>

<section>
	<section data-markdown="slides/cleft_analysis/intro.md"></section>
	<section data-markdown="slides/cleft_analysis/asymmetry.md"></section>
	<section data-markdown="slides/cleft_analysis/size_distribution.md"></section>
</section>

<!--SLIDES END HERE-->

			</div>
		</div>

		<script>
			const titleNode = document.getElementById("title");
			if (titleNode && !titleNode.innerHTML) {
				 titleNode.innerHTML = document.title;
			}

			const authorNode = document.getElementById("author");
			if (authorNode && !authorNode.innerHTML) {
				 for (let meta of document.getElementsByTagName("meta")) {
					  if (meta.getAttribute("name") === "author") {
							authorNode.innerHTML = meta.getAttribute("content");
							break;
					  }
				 }
			}

			const dateNode = document.getElementById("date");
			if (dateNode && !dateNode.innerHTML) {
				 const today = new Date();
				 dateNode.innerHTML = `${today.getFullYear()}-${today.getMonth() + 1}-${today.getDate()}`
			}
		</script>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				transitionSpeed: "fast",
				progress: true,
				controlsTutorial: true,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
