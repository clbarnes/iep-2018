<!doctype html>
<html xmlns:vertical-align="http://www.w3.org/1999/xhtml">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>IEP 2018-06-21</title>
		<meta name="author" content="Chris Barnes">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/janelia.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--Colours-->
		<script>
			HHMIBlue = "#058d96";
			HHMIBlack = "#000";
			HHMIGreenA = "#8ac341";
			HHMIGreenB = "#2da948";
			HHMIWhite = "#fff";
			HHMIGrayA = "#838286";
			HHMIGrayB = "#aaa";
			HHMIGrayC = "#dcdcdc";
		</script>

	</head>
	<body>
		<div class="reveal">
			<!--<header></header>-->
			<div class="slides">

<!--SLIDES START HERE-->

<section class="title-slide" data-background="linear-gradient(to right, #058d96, #8ac341)">
	<div style="display: table; width: 100%; margin-bottom: 20%">
		<div style="width: 50%; display: table-cell; vertical-align: middle">
			<img class="plain" src="img/HHMI_Janelia_White.svg"
				 style="background: none; max-width: 70%; min-height: 50%; vertical-align: middle;
				 width: auto; height: auto; padding: 0; margin: 0;">
		</div>

		<div style="width: 50%; display: table-cell; vertical-align: middle; padding: 0; margin: 0;">
			<h3 id="author" style="text-align: right"></h3>
			<h3 style="text-align: right">Cardona lab</h3>
			<!--<h3 id="date" style="text-align: right; vertical-align: text-bottom; padding: 0; margin: 0;">2018-06-21</h3>-->
		</div>
	</div>
	<h1 id="title"></h1>
	<h2 id="subtitle"></h2>
</section>

<section>
	<section data-markdown="slides/introduction/intro.md"></section>
	<section data-markdown="slides/introduction/connectomes.md"></section>
	<section data-transition="slide-in none-out" data-markdown="slides/introduction/reductions.md"></section>
	<section data-transition="none-in slide-out" data-markdown="slides/introduction/reductions2.md"></section>
</section>

<section>
	<section data-markdown="slides/extrasynaptic_network/intro.md"></section>

	<section>
		<figure>
			<img src="img/fig/hive_plot.png" width="70%">
			<figcaption class="citation"><a href="https://doi.org/10.1371/journal.pcbi.1005283">Bentley et al. (2016).
				The Multilayer Connectome of Caenorhabditis elegans</a></figcaption>
		</figure>

		<aside class="notes">
			<p>This hive plot shows the extent to which the worm's connectivity patterns are potentially affected by
				extrasynaptic monoamine edges.</p>
			<p>Neurons are classified as sensory, motor or interneurons and arrayed on these three axes by their degree
				(number of edges).</p>
			<p>In the physical network of synapses and gap junctions, the hub nodes are interneurons, but in the multiplex
				network sensory and motor neurons take a much larger role.</p>
			<p>This may reflect neuromodulatory neurotransmitters' role in setting a body-wide state, such as hunger, which
				was shown experimentally by Ghosh et al in 2016</p>
			<p>There aren't many organisms whose neurotransmitter repertoire is as conveniently accessed as elegans', but these
				results, along with others published in our paper, suggest that the extrasynaptic connectome is a lot of
			  valuable data to leave on the table.</p>
		</aside>
	</section>

</section>

<section>

	<section data-markdown="slides/cleft_analysis/intro.md"></section>

	<section data-markdown="slides/cleft_analysis/size_analysis.md"></section>

	<section>
		<img src="img/fig/orn-pn-ch-basin.png" alt="ORN -> PN and a1 cho -> Basin">
	</section>

        <section data-transition="none-out">
		<img src="img/fig/cho_basin_synaptic_area_hist.svg">
		<aside class="notes">
			<p>The first question to answer is, of course, how big are these synapses?</p>
			<p>The distribution is approximately normal on a log scale, which is common for measurements of living tissue.</p>
			<p>This is a comforting sanity check.</p>
			<p>90% of synapses fall within approximately 1 order of magnitude of size.</p>
		</aside>
	</section>

	<section data-transition="none-in">
		<img src="img/fig/orn_pn_synaptic_area_hist.svg">
		<aside class="notes">
			<p>Here is the same plot for the ORN-PN layer.</p>
			<p>Notably, the synapses are about 50% larger - this reinforces the variance in synapse morphology between
			cell types well known in other organisms, and therefore the need to distinguish between synapse sizes when
				comparing connectivity patterns within and across circuits.</p>
		</aside>
	</section>

	<section data-markdown="slides/cleft_analysis/asymmetry.md"></section>

	<section>
		<img src="img/fig/cho_basin_single_count_vs_area.svg" width="80%">
		<aside class="notes">
			<p>There's a lot going on in this next plot, so here's a gentle introduction.</p>
			<p>Each point is a single edge - that is, the sum of all synaptic contacts for a single unilateral pair of
				neurons.</p>
			<p>We plot the number of synapses against the total synaptic area of that edge.</p>
			<p>The mirror pair of neurons are in the same color and connected by a dotted line.</p>
		</aside>
	</section>

	<section>
		<img src="img/fig/cho_basin_count_vs_area.svg" width="80%">
		<aside class="notes">
			<p>Here is the same thing fleshed out to include the whole chordotonal-basin layer.</p>
			<p>Notably, the linear regression fits very well and almost intersects at (0, 0),
				which is evidence that for this cell type, synapse size is pretty consistent and that contact number is indeed
			the primary determinant of connection strength.</p>
			<p>You may also note that the lines connecting mirror pairs are largely aligned with the linear regression, which
			suggests that synapse count isn't making up for contralateral asymmetries either.</p>
		</aside>
	</section>

	<section>
		<img src="img/fig/cho_basin_leftright_bias.svg">
		<aside class="notes">
			<p>And to back that point up, here are the left-right asymmetries quantified in terms of count and area.</p>
			<p>The asymmetry quantity is the left value divided by sum of the left and right, normalised to the [-1, 1]
				interval: that is to say, 0 is perfect symmetry, +1 would be unilateral on the left side, and -1 would be
				unilateral on the right.</p>
			<p>The second figure shows the mean of the absolute asymmetry values.</p>
			<p>If synapse size was making up for deficits in synapse number, we'd expect to see the orange bars consistently
				smaller than the blue, but that's not the case.</p>
			<p>The ORN-PN data tells the same story, but with only two pairs of cells on each side there's less data to look
				at.</p>
			<p>From this data, it looks like synapse area is no better a proxy for connection strength than synapse count,
				but I would be interested to see similar data for bilateral neurons projecting to the same neuron.</p>
		</aside>
	</section>

	<section>
		<figure>
			<img src="img/fig/synapse_fractions.png">
			<figcaption class="citation"><a href="https://doi.org/10.7554/eLife.14859">Berck et al. (2016)
				The wiring diagram of a glomerular olfactory system</a></figcaption>
		</figure>
		<aside class="notes">
			<p>One explanation for this is that, from other work, we can see that the raw volume of input from any given cell
			or class is probably less important than what proportion of the cell's total input that class makes up.</p>
			<p>In this paper, it was shown that even though the absolute number of inputs to and from glomerular local
				interneurons have some left-right disparities, the synapse count fraction shows much better parity.</p>
			<p>Using this small data set, synapse area fraction calculations are not possible because every input synapse in the
				entire dendritic field of the postsynaptic partner needs to be annotated, but this will be tractable with
				automation.</p>
		</aside>
	</section>

	<section data-markdown="slides/cleft_analysis/applications.md"></section>

</section>

<section data-markdown="slides/conclusion.md"></section>

<section class="title-slide" data-background="linear-gradient(to right, #058d96, #8ac341)">
	<h1>Acknowledgements</h1>
	<ul>
		<li>Cardona lab</li>
		<li>Scientific software and Ilastik developers</li>
		<li>Janelia/ Cambridge graduate programs</li>
	</ul>
	<aside class="notes">
		<p>I'd like to recognise the Cardona lab, in particular Albert, Andrew Champion and Tom Kazimiers</p>
		<p>Scientific software at Janelia and the developers of Ilastik, for their initiation and
			continued support of the automated synapse detection pipeline</p>
		<p>The graduate program for support for me personally, as hard as I sometimes make it for them</p>
	</aside>
</section>


<section>
	<section data-markdown="slides/tools/intro.md"></section>

	<section data-markdown="slides/tools/miscellaneous.md"></section>

	<section>
		<img src="img/fig/sdv.gif">
	</section>

	<section data-markdown="slides/tools/contributions.md"></section>

	<section data-markdown="slides/tools/l1_export.md"></section>

	<section>
		<img src="img/fig/n5_orthoviews.png" alt="Orthoviews of N5 stack in CATMAID" border="0">
		<aside class="notes">
			<p>H2N5 library which I contributed to, but primarily written by Andrew Champion.</p>
			<p>Not fast enough for the majority of tracing</p>
		</aside>
	</section>

	<section data-markdown="slides/tools/catpy.md"></section>

	<section data-markdown="slides/tools/catmaid_extensions.md"></section>

</section>

<section>

	<section data-markdown="slides/synapse_suggestor/intro.md"></section>

	<section data-markdown="slides/synapse_suggestor/synapse_detection.md"></section>

	<section>
		<img src="img/fig/synapse_thresholding.svg">
		<aside class="notes">
			<p>The question is how accurate the detections are.</p>

			<p>On the right is a precision-recall curve, and it shows that without post-hoc filtering
			we have a very high recall but a low precision (lots of false positives).</p>

			<p>We have statistics on the synapses' size and shape very easily accessible from this regime,
			so the PR curve shows what happens if we filter the synapses by those statistics
			(synaptic volume, contact volume, Z depth, certainty - a percentage output by the pixel classifier),
			and by a logistic regression classifier based on those features.</p>

			<p>Along with the violin plot, we can clearly toss out a lot of false positives with simple thresholding,
			but there are babies in that bathwater.
			At least an understanding of these characteristics would allow a user to pick
			only the most likely new synapses for manual review.</p>
		</aside>
	</section>

	<section data-markdown="slides/synapse_suggestor/catmaid_extension.md"></section>

	<section>
		<img src="img/fig/synapse_suggestor.gif">
		<aside class="notes">

		</aside>
	</section>

	<section>
		<img src="img/fig/raw_pred_widget.png" alt="Debug view" border="0">
	</section>

	<!--<section data-markdown="slides/synapse_suggestor/partner_classification.md"></section>-->

	<!--<section>-->
		<!--<img src="img/fig/data_sample.svg" alt="Sample training data for partner classification">-->
		<!--<p style="background: #000000">-->
			<!--<span style="color: #ff0000">membrane </span>-->
			<!--<span style="color: #00ff00">raw </span>-->
			<!--<span style="color: #0000ff">synapse</span>-->
		<!--</p>-->
		<!--<aside class="notes">-->
			<!--<p>Firstly, we generate the data.</p>-->

			<!--<p>There is a lot of literature already on classifying RGB images, so I wanted to pack our-->
				<!--information cached earlier into 3 channels.</p>-->

			<!--<p>I blacked out regions outside of the synapse and partner we're interested in,-->
				<!--rotated it so that the partner is above the synapse, and fixed the synapse in the middle-bottom of the image.</p>-->

			<!--<ul>-->
				<!--<li>Presynaptic: dense vesicle cloud, T-bar (note segmentation failure)</li>-->
				<!--<li>Postsynaptic: hard to see, but post-synaptic densities</li>-->
				<!--<li>None: ambiguity between membrane and synapse, only corner of synapse</li>-->
			<!--</ul>-->
		<!--</aside>-->
	<!--</section>-->

	<!--<section>-->
		<!--<img src="img/fig/training_accuracy.svg" alt="Validation accuracy during training">-->
		<!--<aside class="notes">-->
			<!--<p>Unfortunately, it didn't pan out - over training epochs we'd hope to see the validation accuracy increasing-->
			<!--from chance (33%) towards 100%.</p>-->

			<!--<p>The ResNet comprises a number of different architectures, primarily distinguished by the number of layers,-->
			<!--but this was pretty much the story for all the ones I tried.</p>-->

			<!--<p>Humans can do this problem, so I'd like to go back and make a more intelligent attempt if time allows - -->
			<!--for example, it doesn't look like the membrane or synapse channels really add much,-->
			<!--and the blacking out may be too strong a signal.</p>-->
		<!--</aside>-->
	<!--</section>-->

</section>

<!--SLIDES END HERE-->

			</div>
		</div>

		<script>
			const titleNode = document.getElementById("title");
			if (titleNode && !titleNode.innerHTML) {
				 titleNode.innerHTML = document.title;
			}

			const authorNode = document.getElementById("author");
			if (authorNode && !authorNode.innerHTML) {
				 for (let meta of document.getElementsByTagName("meta")) {
					  if (meta.getAttribute("name") === "author") {
							authorNode.innerHTML = meta.getAttribute("content");
							break;
					  }
				 }
			}

			function zeroPad(number, length) {return ("0000" + number).slice(-length);}

			const dateNode = document.getElementById("date");
			if (dateNode && !dateNode.innerHTML) {
				 const today = new Date();
				 dateNode.innerHTML = `${zeroPad(today.getFullYear(), 4)}-${zeroPad(today.getMonth() + 1, 2)}-${zeroPad(today.getDate(), 2)}`
			}
		</script>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				transitionSpeed: "fast",
				progress: true,
				controlsTutorial: true,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
